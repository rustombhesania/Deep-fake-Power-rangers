{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy librosa\n",
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ASVspoof19Loader:\n",
    "    def __init__(self, audio_dir, label_file, sample_rate=16000):\n",
    "        self.audio_dir   = audio_dir\n",
    "        self.label_file  = label_file\n",
    "        self.sample_rate = sample_rate\n",
    "        self.label_map   = self.parsed_label_file()\n",
    "\n",
    "    def parsed_label_file(self):\n",
    "        label_dict = {}\n",
    "        with open(self.label_file, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                # ← use the 2nd column (parts[1]) for the actual audio filename\n",
    "                file_id  = parts[1]\n",
    "                filename = file_id + '.flac'\n",
    "                label    = 0 if parts[-1]=='bonafide' else 1\n",
    "                label_dict[filename] = label\n",
    "        return label_dict\n",
    "\n",
    "    def load_audio(self, filepath):\n",
    "        try:\n",
    "            wav, sr = librosa.load(filepath, sr=self.sample_rate)\n",
    "            return wav / (np.max(np.abs(wav)) + 1e-9)\n",
    "        except Exception as e:\n",
    "            print(f\"[Error] {filepath}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def load_dataset(self):\n",
    "        data = []\n",
    "        for fname, label in self.label_map.items():\n",
    "            full_path = os.path.join(self.audio_dir, fname)\n",
    "            if os.path.exists(full_path):\n",
    "                audio = self.load_audio(full_path)\n",
    "                if audio is not None:\n",
    "                    data.append((audio, label))\n",
    "            else:\n",
    "                print(f\"[Warning] Missing: {full_path}\")\n",
    "        return data\n",
    "\n",
    "# ——— Test it ———\n",
    "audio_folder  = r\"C:\\ASVSpoof19\\LA\\ASVspoof2019_LA_train\\flac\"\n",
    "protocol_file = r\"C:\\ASVSpoof19\\LA\\ASVspoof2019_LA_cm_protocols\\ASVspoof2019.LA.cm.train.trn.txt\"\n",
    "\n",
    "loader  = ASVspoof19Loader(audio_folder, protocol_file)\n",
    "dataset = loader.load_dataset()\n",
    "\n",
    "print(\"Total files loaded:\", len(dataset))\n",
    "if dataset:\n",
    "    print(\"Sample waveform:\", dataset[0][0][:10], \"Label:\", dataset[0][1])\n",
    "else:\n",
    "    print(\"No files loaded—check that your protocol and folder really match.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###OKR 3 and OKR4###\n",
    "#Feature extraction\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class FeatureExtractor:\n",
    "    def __init__(self, dataset, sr = 16000):\n",
    "        self.dataset = dataset\n",
    "        self.sr = sr\n",
    "\n",
    "    def extract_mfcc(self, y, n_mfcc = 13): #MFCC\n",
    "        return librosa.feature.mfcc(y=y, sr = self.sr, n_mfcc = n_mfcc)\n",
    "    \n",
    "    def extract_lpc(self, y, order = 16): #LPC\n",
    "        #using a 25ms window\n",
    "        frame = y[:int(self.sr * 0.025)]\n",
    "        return librosa.lpc(y=frame, order=order)\n",
    "    \n",
    "    def extract_cqt(self, y): #CQT\n",
    "        return librosa.cqt(y=y, sr=self.sr)\n",
    "\n",
    "    def extract_log_mel(self, y, n_mels=128): #preferring log mel since it is the standard for audio detection CNNs.\n",
    "        mel = librosa.feature.melspectrogram(y=y, sr=self.sr, n_mels=n_mels)\n",
    "        log_mel = librosa.power_to_db(mel)\n",
    "        return log_mel\n",
    "    \n",
    "    def resize_spec(self, spec, target_shape=(128,256)): #resizing the CNN input to a fixed size(128x256)\n",
    "        tensor = torch.tensor(spec).unsqueeze(0) #shape:(1,freq,time)\n",
    "        if tensor.shape[-1] < target_shape[1]:\n",
    "            pad = target_shape[1] - tensor.shape[-1]\n",
    "            tensor = F.pad(tensor, (0, pad))\n",
    "        else:\n",
    "            tensor = tensor[:, :, :target_shape[1]]\n",
    "        return tensor.numpy()\n",
    "    \n",
    "    def process_all(self):\n",
    "        features = []\n",
    "        for idx, (y, label) in enumerate(self.dataset):\n",
    "            if len(y) < 256:  #padding short audio files\n",
    "                y = np.pad(y, (0,256 - len(y)), mode= 'constant')\n",
    "\n",
    "            mfcc = self.extract_mfcc(y)\n",
    "            cqt = self.extract_cqt(y)\n",
    "            lpc = self.extract_lpc(y)\n",
    "            features.append({\n",
    "                'mfcc' : mfcc,\n",
    "                'lpc' : lpc,\n",
    "                'cqt' : cqt,\n",
    "                'label' : label\n",
    "            })\n",
    "            if idx % 100 == 0:\n",
    "                print(f\"Processed {idx} files\")\n",
    "        return features\n",
    "    \n",
    "    def process_for_cnn(self):\n",
    "        spectrograms = []\n",
    "        labels = []\n",
    "        for idx, (y, label) in enumerate(self.dataset):\n",
    "            if len(y) < 256 :\n",
    "                y = np.pad(y, (0,256 - len(y)), mode = 'constant')\n",
    "            \n",
    "            log_mel = self.extract_log_mel(y)\n",
    "            resized = self.resize_spec(log_mel)\n",
    "\n",
    "            spectrograms.append(resized)\n",
    "            labels.append(label)\n",
    "\n",
    "            if idx % 100 == 0:\n",
    "                print(f'Feed for CNN processes: {idx} files')\n",
    "        \n",
    "        return np.array(spectrograms), np.array(labels)\n",
    "\n",
    "\n",
    "audio_folder  = r\"C:\\ASVSpoof19\\LA\\ASVspoof2019_LA_train\\flac\"\n",
    "protocol_file = r\"C:\\ASVSpoof19\\LA\\ASVspoof2019_LA_cm_protocols\\ASVspoof2019.LA.cm.train.trn.txt\"\n",
    "\n",
    "# Run loader from OKR 1\n",
    "loader = ASVspoof19Loader(audio_folder, protocol_file)\n",
    "dataset = loader.load_dataset()\n",
    "\n",
    "#runnign feature extraction\n",
    "extractor = FeatureExtractor(dataset)\n",
    "features = extractor.process_all()\n",
    "\n",
    "#CNN feed processing\n",
    "cnn_specs, cnn_labels = extractor.process_for_cnn()\n",
    "\n",
    "#saving extracted features\n",
    "np.save(\"features.npy\", features)\n",
    "\n",
    "#saving processed feed for CNN\n",
    "np.save(\"cnn_spectrogram.npy\", cnn_specs) #shape: (N,1,128,256)\n",
    "np.save(\"cnn_labels.npy\", cnn_labels)\n",
    "\n",
    "#for KR4\n",
    "print(f'saved {len(cnn_specs)} lpg mel spectrograms for CNN feed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
