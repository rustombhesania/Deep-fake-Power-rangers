{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff9148ae-2497-4dd9-b221-bfc0305e672d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_49189/3022548915.py:37: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y, _ = librosa.load(path, sr=self.sr)\n",
      "/tmp/ipykernel_49189/3022548915.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  features = [torch.tensor(f) for f in features]\n",
      "Training: 100%|██████████████████████████████████████████| 1/1 [00:00<00:00, 129.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading /path/to/audio2.wav: [Errno 2] No such file or directory: '/path/to/audio2.wav'\n",
      "Error loading /path/to/audio1.wav: [Errno 2] No such file or directory: '/path/to/audio1.wav'\n",
      "Training Loss: 0.7057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Dataset class to handle audio loading and feature extraction\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, file_list, label_dict, sr=16000, max_len=16000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            file_list: List of file paths to audio files.\n",
    "            label_dict: Dictionary mapping file names to labels.\n",
    "            sr: Sampling rate.\n",
    "            max_len: Maximum length of audio in samples (padding/truncating length).\n",
    "        \"\"\"\n",
    "        self.file_list = file_list\n",
    "        self.label_dict = label_dict\n",
    "        self.sr = sr\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get audio file path\n",
    "        path = self.file_list[idx]\n",
    "        # Get the corresponding label from label dictionary\n",
    "        label = self.label_dict[os.path.basename(path)]\n",
    "        \n",
    "        # Load the audio file using librosa\n",
    "        try:\n",
    "            y, _ = librosa.load(path, sr=self.sr)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {path}: {e}\")\n",
    "            y = np.zeros(self.max_len)  # If error, return a zero-filled array\n",
    "\n",
    "        # Truncate or pad the audio signal\n",
    "        if len(y) > self.max_len:\n",
    "            y = y[:self.max_len]\n",
    "        else:\n",
    "            y = np.pad(y, (0, self.max_len - len(y)))\n",
    "\n",
    "        # Extract MFCC features (13 MFCCs by default)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=self.sr, n_mfcc=13)\n",
    "        mfcc = torch.tensor(mfcc, dtype=torch.float32)\n",
    "        \n",
    "        return mfcc.T, torch.tensor(label, dtype=torch.long)  # Transpose for time x features\n",
    "\n",
    "# Custom collate function to pad feature sequences\n",
    "def collate_fn(batch):\n",
    "    features, labels = zip(*batch)\n",
    "    features = [torch.tensor(f) for f in features]\n",
    "    features_padded = pad_sequence(features, batch_first=True)  # Shape: [batch_size, max_seq_len, features]\n",
    "    labels = torch.stack(labels)  # Stack labels into a tensor\n",
    "    return features_padded, labels\n",
    "\n",
    "# Example file list and label dictionary (you will provide these)\n",
    "file_list = [\"/path/to/audio1.wav\", \"/path/to/audio2.wav\"]  # Replace with actual paths\n",
    "label_dict = {\"audio1.wav\": 0, \"audio2.wav\": 1}  # Example labels\n",
    "\n",
    "# Create the dataset\n",
    "dataset = AudioDataset(file_list=file_list, label_dict=label_dict, sr=16000, max_len=16000)\n",
    "\n",
    "# DataLoader to batch the data\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Define a simple neural network model\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, 128, batch_first=True)\n",
    "        self.fc = nn.Linear(128, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        x = lstm_out[:, -1, :]  # Take the last time step output\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model, criterion, and optimizer\n",
    "model = SimpleModel(input_dim=13, output_dim=2)  # 13 MFCCs as input, 2 output classes\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "def train(model, dataloader, criterion, optimizer, device):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Iterate over batches\n",
    "    for x, y in tqdm(dataloader, desc=\"Training\"):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        # Zero gradients, forward pass, calculate loss, and backward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Training Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Run training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train(model, dataloader, criterion, optimizer, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21dd1870-72d1-48e4-8cc8-62ae646b0e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: soundfile in /home/rben10/anaconda3/envs/team_labs/lib/python3.11/site-packages (0.13.1)\n",
      "Requirement already satisfied: audioread in /home/rben10/anaconda3/envs/team_labs/lib/python3.11/site-packages (3.0.1)\n",
      "Requirement already satisfied: cffi>=1.0 in /home/rben10/anaconda3/envs/team_labs/lib/python3.11/site-packages (from soundfile) (1.17.1)\n",
      "Requirement already satisfied: numpy in /home/rben10/anaconda3/envs/team_labs/lib/python3.11/site-packages (from soundfile) (2.2.5)\n",
      "Requirement already satisfied: pycparser in /home/rben10/anaconda3/envs/team_labs/lib/python3.11/site-packages (from cffi>=1.0->soundfile) (2.21)\n"
     ]
    }
   ],
   "source": [
    "!pip install soundfile audioread\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66123201-f034-4375-b386-c010516a13d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_49189/4047145467.py:37: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y, _ = librosa.load(path, sr=self.sr)\n",
      "Training: 100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 35.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading ./data/audio1.wav: [Errno 2] No such file or directory: './data/audio1.wav'\n",
      "Error loading ./data/audio2.wav: [Errno 2] No such file or directory: './data/audio2.wav'\n",
      "Training Loss: 0.7692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Dataset class to handle audio loading and feature extraction\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, file_list, label_dict, sr=16000, max_len=16000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            file_list: List of file paths to audio files.\n",
    "            label_dict: Dictionary mapping file names to labels.\n",
    "            sr: Sampling rate.\n",
    "            max_len: Maximum length of audio in samples (padding/truncating length).\n",
    "        \"\"\"\n",
    "        self.file_list = file_list\n",
    "        self.label_dict = label_dict\n",
    "        self.sr = sr\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get audio file path\n",
    "        path = self.file_list[idx]\n",
    "        # Get the corresponding label from label dictionary\n",
    "        label = self.label_dict[os.path.basename(path)]\n",
    "        \n",
    "        # Load the audio file using librosa\n",
    "        try:\n",
    "            y, _ = librosa.load(path, sr=self.sr)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {path}: {e}\")\n",
    "            y = np.zeros(self.max_len)  # If error, return a zero-filled array\n",
    "\n",
    "        # Truncate or pad the audio signal\n",
    "        if len(y) > self.max_len:\n",
    "            y = y[:self.max_len]\n",
    "        else:\n",
    "            y = np.pad(y, (0, self.max_len - len(y)))\n",
    "\n",
    "        # Extract MFCC features (13 MFCCs by default)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=self.sr, n_mfcc=13)\n",
    "        mfcc = torch.tensor(mfcc, dtype=torch.float32)\n",
    "        \n",
    "        return mfcc.T, torch.tensor(label, dtype=torch.long)  # Transpose for time x features\n",
    "\n",
    "# Custom collate function to pad feature sequences\n",
    "def collate_fn(batch):\n",
    "    features, labels = zip(*batch)\n",
    "    features = [f.detach().clone() for f in features]  # Fix for the tensor warning\n",
    "    features_padded = pad_sequence(features, batch_first=True)  # Shape: [batch_size, max_seq_len, features]\n",
    "    labels = torch.stack(labels)  # Stack labels into a tensor\n",
    "    return features_padded, labels\n",
    "\n",
    "# Example file list and label dictionary (replace these with actual files)\n",
    "file_list = [\"./data/audio1.wav\", \"./data/audio2.wav\"]  # Replace with actual paths\n",
    "label_dict = {\"audio1.wav\": 0, \"audio2.wav\": 1}  # Example labels\n",
    "\n",
    "# Create the dataset\n",
    "dataset = AudioDataset(file_list=file_list, label_dict=label_dict, sr=16000, max_len=16000)\n",
    "\n",
    "# DataLoader to batch the data\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Define a simple neural network model\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, 128, batch_first=True)\n",
    "        self.fc = nn.Linear(128, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        x = lstm_out[:, -1, :]  # Take the last time step output\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model, criterion, and optimizer\n",
    "model = SimpleModel(input_dim=13, output_dim=2)  # 13 MFCCs as input, 2 output classes\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "def train(model, dataloader, criterion, optimizer, device):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Iterate over batches\n",
    "    for x, y in tqdm(dataloader, desc=\"Training\"):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        # Zero gradients, forward pass, calculate loss, and backward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Training Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Run training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train(model, dataloader, criterion, optimizer, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8ddb0b-77d5-4d02-b01e-4eaee8dcaf30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
