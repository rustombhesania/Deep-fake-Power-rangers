{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c693373d-301e-4273-9475-c3ed7f282622",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [10,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [11,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [12,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [13,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [14,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [15,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [16,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [17,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [18,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [19,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [20,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [21,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [22,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [23,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [24,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [25,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [26,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [27,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [28,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [29,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [30,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [31,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "Training:   0%|                                                 | 0/794 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_8869771.flac: Error loading audio file: failed to open file /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_8869771.flac\n",
      "Error loading /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_5968863.flac: Error loading audio file: failed to open file /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_5968863.flac\n",
      "Error loading /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_8697779.flac: Error loading audio file: failed to open file /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_8697779.flac\n",
      "Error loading /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_7574942.flac: Error loading audio file: failed to open file /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_7574942.flac\n",
      "Error loading /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_3101729.flac: Error loading audio file: failed to open file /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_3101729.flac\n",
      "Error loading /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_5587807.flac: Error loading audio file: failed to open file /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_5587807.flac\n",
      "Error loading /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_7130400.flac: Error loading audio file: failed to open file /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_7130400.flac\n",
      "Error loading /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_5211999.flac: Error loading audio file: failed to open file /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_5211999.flac\n",
      "Error loading /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_8115679.flac: Error loading audio file: failed to open file /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_8115679.flac\n",
      "Error loading /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_6802522.flac: Error loading audio file: failed to open file /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_6802522.flac\n",
      "Error loading /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_6993085.flac: Error loading audio file: failed to open file /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_6993085.flac\n",
      "Error loading /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_8125498.flac: Error loading audio file: failed to open file /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_8125498.flac\n",
      "Error loading /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_8490370.flac: Error loading audio file: failed to open file /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_8490370.flac\n",
      "Error loading /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_3872127.flac: Error loading audio file: failed to open file /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_3872127.flac\n",
      "Error loading /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_9009599.flac: Error loading audio file: failed to open file /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_9009599.flac\n",
      "Error loading /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_5113958.flac: Error loading audio file: failed to open file /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_5113958.flac\n",
      "Error loading /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_1646363.flac: Error loading audio file: failed to open file /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_1646363.flac\n",
      "Error loading /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_2354581.flac: Error loading audio file: failed to open file /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_2354581.flac\n",
      "Error loading /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_9975703.flac: Error loading audio file: failed to open file /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_9975703.flac\n",
      "Error loading /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_8719032.flac: Error loading audio file: failed to open file /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_8719032.flac\n",
      "Error loading /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_1857723.flac: Error loading audio file: failed to open file /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_1857723.flac\n",
      "Error loading /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_1167935.flac: Error loading audio file: failed to open file /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_1167935.flac\n",
      "Error loading /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_1782897.flac: Error loading audio file: failed to open file /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_1782897.flac\n",
      "Error loading /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_2480249.flac: Error loading audio file: failed to open file /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_2480249.flac\n",
      "Error loading /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_6086880.flac: Error loading audio file: failed to open file /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_6086880.flac\n",
      "Error loading /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_5002945.flac: Error loading audio file: failed to open file /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_5002945.flac\n",
      "Error loading /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_6080026.flac: Error loading audio file: failed to open file /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_6080026.flac\n",
      "Error loading /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_9777101.flac: Error loading audio file: failed to open file /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_9777101.flac\n",
      "Error loading /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_5595576.flac: Error loading audio file: failed to open file /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_5595576.flac\n",
      "Error loading /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_1575776.flac: Error loading audio file: failed to open file /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_1575776.flac\n",
      "Error loading /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_8049551.flac: Error loading audio file: failed to open file /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_8049551.flac\n",
      "Error loading /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_1775438.flac: Error loading audio file: failed to open file /home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac/LA_T_1775438.flac\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 113\u001b[39m\n\u001b[32m    110\u001b[39m criterion = nn.CrossEntropyLoss()\n\u001b[32m    111\u001b[39m optimizer = optim.Adam(model.parameters(), lr=\u001b[32m0.001\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m train(model, dataloader, criterion, optimizer, device)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 91\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, dataloader, criterion, optimizer, device)\u001b[39m\n\u001b[32m     88\u001b[39m loss = criterion(outputs, y)\n\u001b[32m     90\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m loss.backward()\n\u001b[32m     92\u001b[39m optimizer.step()\n\u001b[32m     94\u001b[39m total_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/team_labs/lib/python3.11/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m torch.autograd.backward(\n\u001b[32m    649\u001b[39m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs=inputs\n\u001b[32m    650\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/team_labs/lib/python3.11/site-packages/torch/autograd/__init__.py:346\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    337\u001b[39m inputs = (\n\u001b[32m    338\u001b[39m     (inputs,)\n\u001b[32m    339\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, (torch.Tensor, graph.GradientEdge))\n\u001b[32m   (...)\u001b[39m\u001b[32m    342\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m ()\n\u001b[32m    343\u001b[39m )\n\u001b[32m    345\u001b[39m grad_tensors_ = _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/team_labs/lib/python3.11/site-packages/torch/autograd/__init__.py:221\u001b[39m, in \u001b[36m_make_grads\u001b[39m\u001b[34m(outputs, grads, is_grads_batched)\u001b[39m\n\u001b[32m    218\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    219\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, torch.Tensor)\n\u001b[32m    220\u001b[39m         new_grads.append(\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m             torch.ones_like(out, memory_format=torch.preserve_format)\n\u001b[32m    222\u001b[39m         )\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    224\u001b[39m     new_grads.append(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ASVspoofDataset(Dataset):\n",
    "    def __init__(self, protocol_file, flac_dir, sample_size=16000, device='cpu'):\n",
    "        self.protocol_file = protocol_file\n",
    "        self.flac_dir = flac_dir\n",
    "        self.sample_size = sample_size\n",
    "        self.device = device\n",
    "        self.data = self._load_protocol()\n",
    "\n",
    "        # Define MFCC transform using torchaudio\n",
    "        self.mfcc_transform = torchaudio.transforms.MFCC(\n",
    "            sample_rate=16000,\n",
    "            n_mfcc=13,\n",
    "            melkwargs={'n_fft': 400, 'hop_length': 160, 'n_mels': 40}\n",
    "        ).to(self.device)\n",
    "\n",
    "    def _load_protocol(self):\n",
    "        with open(self.protocol_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        data = []\n",
    "        for line in lines:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) < 5:\n",
    "                continue\n",
    "            file_name = parts[1]\n",
    "            label = 0 if parts[4] == 'bonafide' else 1\n",
    "            data.append((file_name, label))\n",
    "        return data\n",
    "\n",
    "    def _pad_audio(self, audio):\n",
    "        if audio.size(1) < self.sample_size:\n",
    "            pad_size = self.sample_size - audio.size(1)\n",
    "            audio = torch.nn.functional.pad(audio, (0, pad_size))\n",
    "        else:\n",
    "            audio = audio[:, :self.sample_size]\n",
    "        return audio\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name, label = self.data[idx]\n",
    "        path = os.path.join(self.flac_dir, f\"{file_name}.flac\")\n",
    "\n",
    "        try:\n",
    "            waveform, sample_rate = torchaudio.load(path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {path}: {e}\")\n",
    "            return torch.zeros((13, self.sample_size // 160)), -1\n",
    "\n",
    "        waveform = self._pad_audio(waveform)\n",
    "        waveform = waveform.to(self.device)\n",
    "\n",
    "        mfcc = self.mfcc_transform(waveform)\n",
    "        mfcc = mfcc.squeeze(0)\n",
    "\n",
    "        return mfcc, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "class SimpleDNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, output_dim=2):\n",
    "        super(SimpleDNN, self).__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)\n",
    "\n",
    "def train(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x, y in tqdm(dataloader, desc=\"Training\"):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        x = x.mean(dim=-1)  # Average over time dimension\n",
    "        outputs = model(x)\n",
    "        loss = criterion(outputs, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Training loss: {total_loss / len(dataloader):.4f}\")\n",
    "\n",
    "# Configs\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "protocol_path = \"/home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.train.trn.txt\"\n",
    "flac_dir = \"/home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac\"\n",
    "sample_size = 16000  # 1 second of audio\n",
    "\n",
    "# Load data\n",
    "dataset = ASVspoofDataset(protocol_path, flac_dir, sample_size=sample_size, device=device)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Build and train model\n",
    "model = SimpleDNN(input_dim=13).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train(model, dataloader, criterion, optimizer, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b97b7f8-3824-4b91-a83b-a42248e61631",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 163\u001b[39m\n\u001b[32m    160\u001b[39m sample_size = \u001b[32m16000\u001b[39m  \u001b[38;5;66;03m# 1 second of audio\u001b[39;00m\n\u001b[32m    162\u001b[39m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m dataset = ASVspoofDataset(protocol_path, flac_dir, sample_size=sample_size, device=device)\n\u001b[32m    164\u001b[39m dataloader = DataLoader(dataset, batch_size=\u001b[32m32\u001b[39m, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers=\u001b[32m4\u001b[39m)\n\u001b[32m    166\u001b[39m \u001b[38;5;66;03m# Build and train model\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mASVspoofDataset.__init__\u001b[39m\u001b[34m(self, protocol_file, flac_dir, sample_size, device)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mself\u001b[39m.data = \u001b[38;5;28mself\u001b[39m._load_protocol()\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Define MFCC transform using torchaudio\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;28mself\u001b[39m.mfcc_transform = torchaudio.transforms.MFCC(\n\u001b[32m     26\u001b[39m     sample_rate=\u001b[32m16000\u001b[39m,\n\u001b[32m     27\u001b[39m     n_mfcc=\u001b[32m13\u001b[39m,\n\u001b[32m     28\u001b[39m     melkwargs={\u001b[33m'\u001b[39m\u001b[33mn_fft\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m400\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mhop_length\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m160\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mn_mels\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m40\u001b[39m}\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m ).to(\u001b[38;5;28mself\u001b[39m.device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/team_labs/lib/python3.11/site-packages/torch/nn/modules/module.py:1355\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1352\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1353\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1355\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._apply(convert)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/team_labs/lib/python3.11/site-packages/torch/nn/modules/module.py:915\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m         module._apply(fn)\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    919\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    920\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    926\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/team_labs/lib/python3.11/site-packages/torch/nn/modules/module.py:915\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m         module._apply(fn)\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    919\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    920\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    926\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/team_labs/lib/python3.11/site-packages/torch/nn/modules/module.py:1003\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m   1001\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, buf \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._buffers.items():\n\u001b[32m   1002\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1003\u001b[39m         \u001b[38;5;28mself\u001b[39m._buffers[key] = fn(buf)\n\u001b[32m   1005\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/team_labs/lib/python3.11/site-packages/torch/nn/modules/module.py:1341\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1334\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1335\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1336\u001b[39m             device,\n\u001b[32m   1337\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1338\u001b[39m             non_blocking,\n\u001b[32m   1339\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1340\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1341\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1342\u001b[39m         device,\n\u001b[32m   1343\u001b[39m         dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1344\u001b[39m         non_blocking,\n\u001b[32m   1345\u001b[39m     )\n\u001b[32m   1346\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1347\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ASVspoofDataset(Dataset):\n",
    "    def __init__(self, protocol_file, flac_dir, sample_size=16000, device='cpu'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            protocol_file (str): Path to the protocol file containing file names and labels.\n",
    "            flac_dir (str): Directory where FLAC audio files are stored.\n",
    "            sample_size (int): Target sample size for the audio (default: 16000 for 1 second).\n",
    "            device (str): The device to store the data ('cpu' or 'cuda').\n",
    "        \"\"\"\n",
    "        self.protocol_file = protocol_file\n",
    "        self.flac_dir = flac_dir\n",
    "        self.sample_size = sample_size\n",
    "        self.device = device\n",
    "        self.data = self._load_protocol()\n",
    "\n",
    "        # Define MFCC transform using torchaudio\n",
    "        self.mfcc_transform = torchaudio.transforms.MFCC(\n",
    "            sample_rate=16000,\n",
    "            n_mfcc=13,\n",
    "            melkwargs={'n_fft': 400, 'hop_length': 160, 'n_mels': 40}\n",
    "        ).to(self.device)\n",
    "\n",
    "    def _load_protocol(self):\n",
    "        \"\"\"\n",
    "        Load the protocol file containing file names and labels.\n",
    "        \"\"\"\n",
    "        with open(self.protocol_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        data = []\n",
    "        for line in lines:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) < 5:\n",
    "                continue\n",
    "            file_name = parts[1]\n",
    "            label = 0 if parts[4] == 'bonafide' else 1\n",
    "            data.append((file_name, label))\n",
    "        return data\n",
    "\n",
    "    def _pad_audio(self, audio):\n",
    "        \"\"\"\n",
    "        Pad or truncate the audio to the target sample size.\n",
    "        \"\"\"\n",
    "        if audio.size(1) < self.sample_size:\n",
    "            pad_size = self.sample_size - audio.size(1)\n",
    "            audio = torch.nn.functional.pad(audio, (0, pad_size))\n",
    "        else:\n",
    "            audio = audio[:, :self.sample_size]\n",
    "        return audio\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Fetch the waveform and corresponding label for a given index.\n",
    "        \"\"\"\n",
    "        file_name, label = self.data[idx]\n",
    "        path = os.path.join(self.flac_dir, f\"{file_name}.flac\")\n",
    "\n",
    "        try:\n",
    "            waveform, sample_rate = torchaudio.load(path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {path}: {e}\")\n",
    "            # Return a tensor of zeros (representing failed load)\n",
    "            return torch.zeros((13, self.sample_size // 160)), -1\n",
    "\n",
    "        waveform = self._pad_audio(waveform)\n",
    "        waveform = waveform.to(self.device)\n",
    "\n",
    "        mfcc = self.mfcc_transform(waveform)\n",
    "        mfcc = mfcc.squeeze(0)  # Remove channel dimension if it's 1\n",
    "\n",
    "        return mfcc, label\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the total number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "class SimpleDNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, output_dim=2):\n",
    "        \"\"\"\n",
    "        Simple feed-forward neural network for classification.\n",
    "        \n",
    "        Args:\n",
    "            input_dim (int): The input dimension for the first layer (13 for MFCC features).\n",
    "            hidden_dim (int): Number of neurons in the hidden layer.\n",
    "            output_dim (int): Number of output classes (2: bonafide, spoof).\n",
    "        \"\"\"\n",
    "        super(SimpleDNN, self).__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "        \"\"\"\n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "def train(model, dataloader, criterion, optimizer, device):\n",
    "    \"\"\"\n",
    "    Training loop.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The model to train.\n",
    "        dataloader (DataLoader): DataLoader providing batches of data.\n",
    "        criterion (nn.Module): The loss function.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer.\n",
    "        device (str): The device ('cpu' or 'cuda').\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "\n",
    "    # Loop through the training data\n",
    "    for x, y in tqdm(dataloader, desc=\"Training\"):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # Average over time (dim=-1)\n",
    "        x = x.mean(dim=-1)  # MFCC features averaged across time\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(x)\n",
    "        loss = criterion(outputs, y)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Loss and accuracy calculation\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_preds += (predicted == y).sum().item()\n",
    "        total_preds += y.size(0)\n",
    "\n",
    "    # Print loss and accuracy\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = 100 * correct_preds / total_preds\n",
    "    print(f\"Training loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Configs\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "protocol_path = \"/home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.train.trn.txt\"\n",
    "flac_dir = \"/home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac\"\n",
    "sample_size = 16000  # 1 second of audio\n",
    "\n",
    "# Load data\n",
    "dataset = ASVspoofDataset(protocol_path, flac_dir, sample_size=sample_size, device=device)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "\n",
    "# Build and train model\n",
    "model = SimpleDNN(input_dim=13).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "train(model, dataloader, criterion, optimizer, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d06b01b-0073-49d3-81c2-ba7bbee4af7a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 194\u001b[39m\n\u001b[32m    191\u001b[39m sample_size = \u001b[32m16000\u001b[39m  \u001b[38;5;66;03m# 1 second of audio\u001b[39;00m\n\u001b[32m    193\u001b[39m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m dataset = ASVspoofDataset(protocol_path, flac_dir, sample_size=sample_size, device=device)\n\u001b[32m    195\u001b[39m dataloader = DataLoader(dataset, batch_size=\u001b[32m32\u001b[39m, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    197\u001b[39m \u001b[38;5;66;03m# Build and train model\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mASVspoofDataset.__init__\u001b[39m\u001b[34m(self, protocol_file, flac_dir, sample_size, device)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mself\u001b[39m.data = \u001b[38;5;28mself\u001b[39m._load_protocol()\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Define MFCC transform using torchaudio\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;28mself\u001b[39m.mfcc_transform = torchaudio.transforms.MFCC(\n\u001b[32m     28\u001b[39m     sample_rate=\u001b[32m16000\u001b[39m,\n\u001b[32m     29\u001b[39m     n_mfcc=\u001b[32m13\u001b[39m,\n\u001b[32m     30\u001b[39m     melkwargs={\u001b[33m'\u001b[39m\u001b[33mn_fft\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m400\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mhop_length\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m160\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mn_mels\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m40\u001b[39m}\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m ).to(\u001b[38;5;28mself\u001b[39m.device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/team_labs/lib/python3.11/site-packages/torch/nn/modules/module.py:1355\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1352\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1353\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1355\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._apply(convert)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/team_labs/lib/python3.11/site-packages/torch/nn/modules/module.py:915\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m         module._apply(fn)\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    919\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    920\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    926\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/team_labs/lib/python3.11/site-packages/torch/nn/modules/module.py:915\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m         module._apply(fn)\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    919\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    920\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    926\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/team_labs/lib/python3.11/site-packages/torch/nn/modules/module.py:1003\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m   1001\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, buf \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._buffers.items():\n\u001b[32m   1002\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1003\u001b[39m         \u001b[38;5;28mself\u001b[39m._buffers[key] = fn(buf)\n\u001b[32m   1005\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/team_labs/lib/python3.11/site-packages/torch/nn/modules/module.py:1341\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1334\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1335\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1336\u001b[39m             device,\n\u001b[32m   1337\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1338\u001b[39m             non_blocking,\n\u001b[32m   1339\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1340\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1341\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1342\u001b[39m         device,\n\u001b[32m   1343\u001b[39m         dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1344\u001b[39m         non_blocking,\n\u001b[32m   1345\u001b[39m     )\n\u001b[32m   1346\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1347\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ASVspoofDataset(Dataset):\n",
    "    def __init__(self, protocol_file, flac_dir, sample_size=16000, device='cpu'):\n",
    "        \"\"\"\n",
    "        Dataset class for loading the ASVspoof dataset.\n",
    "\n",
    "        Args:\n",
    "            protocol_file (str): Path to the protocol file.\n",
    "            flac_dir (str): Directory containing the FLAC audio files.\n",
    "            sample_size (int): The fixed size to pad or truncate the audio.\n",
    "            device (str): The device to run the model on ('cpu' or 'cuda').\n",
    "        \"\"\"\n",
    "        self.protocol_file = protocol_file\n",
    "        self.flac_dir = flac_dir\n",
    "        self.sample_size = sample_size\n",
    "        self.device = device\n",
    "        self.data = self._load_protocol()\n",
    "\n",
    "        # Define MFCC transform using torchaudio\n",
    "        self.mfcc_transform = torchaudio.transforms.MFCC(\n",
    "            sample_rate=16000,\n",
    "            n_mfcc=13,\n",
    "            melkwargs={'n_fft': 400, 'hop_length': 160, 'n_mels': 40}\n",
    "        ).to(self.device)\n",
    "\n",
    "    def _load_protocol(self):\n",
    "        \"\"\"\n",
    "        Load the protocol file and extract the filenames and labels.\n",
    "\n",
    "        Returns:\n",
    "            data (list): List of tuples (filename, label)\n",
    "        \"\"\"\n",
    "        with open(self.protocol_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        data = []\n",
    "        for line in lines:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) < 5:\n",
    "                continue\n",
    "            file_name = parts[1]\n",
    "            label = 0 if parts[4] == 'bonafide' else 1  # bonafide -> 0, spoof -> 1\n",
    "            data.append((file_name, label))\n",
    "        return data\n",
    "\n",
    "    def _pad_audio(self, audio):\n",
    "        \"\"\"\n",
    "        Pad or truncate the audio to a fixed sample size.\n",
    "\n",
    "        Args:\n",
    "            audio (Tensor): Audio waveform tensor.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Padded or truncated audio tensor.\n",
    "        \"\"\"\n",
    "        if audio.size(1) < self.sample_size:\n",
    "            pad_size = self.sample_size - audio.size(1)\n",
    "            audio = torch.nn.functional.pad(audio, (0, pad_size))\n",
    "        else:\n",
    "            audio = audio[:, :self.sample_size]\n",
    "        return audio\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Fetch the waveform and corresponding label for a given index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the item to fetch.\n",
    "\n",
    "        Returns:\n",
    "            Tuple: (MFCC features, label) or None if there's an error.\n",
    "        \"\"\"\n",
    "        file_name, label = self.data[idx]\n",
    "        path = os.path.join(self.flac_dir, f\"{file_name}.flac\")\n",
    "\n",
    "        try:\n",
    "            waveform, sample_rate = torchaudio.load(path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {path}: {e}\")\n",
    "            # Skip invalid files by returning None\n",
    "            return None\n",
    "\n",
    "        waveform = self._pad_audio(waveform)\n",
    "        waveform = waveform.to(self.device)\n",
    "\n",
    "        # Apply MFCC transform\n",
    "        mfcc = self.mfcc_transform(waveform)\n",
    "        mfcc = mfcc.squeeze(0)  # Remove the channel dimension if it's 1\n",
    "\n",
    "        return mfcc, label\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the number of samples in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: Number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "class SimpleDNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, output_dim=2):\n",
    "        \"\"\"\n",
    "        Simple fully connected neural network for classification.\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): The dimension of the input features.\n",
    "            hidden_dim (int): The number of units in the hidden layer.\n",
    "            output_dim (int): The number of output classes.\n",
    "        \"\"\"\n",
    "        super(SimpleDNN, self).__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input features.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Predicted class probabilities.\n",
    "        \"\"\"\n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "def train(model, dataloader, criterion, optimizer, device):\n",
    "    \"\"\"\n",
    "    Training loop.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to train.\n",
    "        dataloader (DataLoader): DataLoader providing batches of data.\n",
    "        criterion (nn.Module): The loss function.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer.\n",
    "        device (str): The device ('cpu' or 'cuda').\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "\n",
    "    # Loop through the training data\n",
    "    for x, y in tqdm(dataloader, desc=\"Training\"):\n",
    "        if x is None:  # Skip invalid data points\n",
    "            continue\n",
    "\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # Average over time (dim=-1)\n",
    "        x = x.mean(dim=-1)  # MFCC features averaged across time\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(x)\n",
    "        loss = criterion(outputs, y)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Loss and accuracy calculation\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_preds += (predicted == y).sum().item()\n",
    "        total_preds += y.size(0)\n",
    "\n",
    "    # Print loss and accuracy\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = 100 * correct_preds / total_preds\n",
    "    print(f\"Training loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "# Configs\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "protocol_path = \"/home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.train.trn.txt\"\n",
    "flac_dir = \"/home/rben10/team_labs/ASVSpoof19/LA/ASVspoof2019_LA_train/flac\"\n",
    "sample_size = 16000  # 1 second of audio\n",
    "\n",
    "# Load data\n",
    "dataset = ASVspoofDataset(protocol_path, flac_dir, sample_size=sample_size, device=device)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Build and train model\n",
    "model = SimpleDNN(input_dim=13).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train(model, dataloader, criterion, optimizer, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7016b6f5-07a0-4a99-81c7-8b03cc1698f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
