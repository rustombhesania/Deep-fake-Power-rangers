The prompt : whether it was possible to remove any of the features without affecting the robustness and accuracy of the model.(To avoid unnecessary training time)

Yes â€” it's absolutely possible to remove one or more features from your multi-branch deepfake detection model without severely hurting performance, but which one you remove matters.

Letâ€™s evaluate each of your features and decide whatâ€™s safely removable based on:

Information overlap

Signal uniqueness

Common usage in SOTA models

âœ… Your Feature Candidates
Feature	What It Captures	Common in SOTA?	Redundant With	Risk if Removed
Log-Mel	Perceptual audio content (human hearing)	âœ… Yes	CQT, MFCC (partially)	âŒ High risk â€” do not remove
CQT	Harmonic structure, pitch spacing	âš ï¸ Sometimes	Log-Mel	âœ… Lowâ€“Medium risk
MFCC	Spectral envelope, formant structure	âœ… Yes	Log-Mel	âœ… Lowâ€“Medium risk
LPC	Vocal tract shape / source-filter model	âŒ Rare	MFCC	âœ…âœ… Very low risk

ğŸ§  Recommendation
ğŸ”´ DO NOT remove:
Log-Mel: this is the most informative and robust representation for CNNs. All other features are somewhat derived from or approximating what log-Mel already captures.

ğŸŸ¡ You CAN safely remove:
LPC â€” itâ€™s a linear model of the vocal tract, but:

Itâ€™s redundant with MFCC

Not used in most deepfake SOTA models

Can be noisy for short or low-energy speech

ğŸŸ¡ You MIGHT remove either MFCC or CQT:
Removing CQT: okay if youâ€™re less focused on pitch and harmonic artifacts

Removing MFCC: okay if you retain log-Mel + CQT, since MFCC is derived from Mel-spectra anyway

âœ… Simplified Two-Branch Baseline (Safe and Effective)
Log-Mel (CNN branch)

MFCC (small CNN or FC branch)

â†’ merge â†’ classifier

This would still be very effective, and much lighter than a full 4-branch model.

âœ… Summary
If you want to keep onlyâ€¦	Tradeoff
Log-Mel + MFCC	Keeps spectral + perceptual cues
Log-Mel + CQT	Keeps perceptual + harmonic info
Log-Mel only	Lightest + still strong baseline
Remove LPC first	Least impact
